# RAG Faciliatated Medical Question Answering

## Summary

**This project aims to investigate the effectiveness of applying retrieval-augmented generation (RAG) on medical question answering**. **MedQuAD**, a **medical question answer dataset** containing more than **200 thousand question-answer pairs**, was used for the study. **With correctly paired question-document as input**, the **test accuracy and F-1 score achieved 74.49 and 81.41**, higher than accuracy and achieved using questions alone (63.71 and 77.01 respectively).  The **ROUGE-L and BLEU scores** using the correct question-document pair as input **were 12.66 and 13.72**, higher than using questions with retrieved documents and questions only as input. **The results show the potential of RAG to improve the performance of machine medical question answering**. Further research is needed to increase accuracy for clinical use.

## Background

Medical question answering can benefit caregivers by reducing workload and providing actionable insights. **The challenge of medical question answering is to generate accurate responses** using natural human languages. Recent advances in large language models (LLMs) have dramatically enhanced machine capacity to generate natural language. However, these language models can generate counterfactual responses, for example, a recommended procedure to a medical condition that is perfect in language usage but does not help address the issue faced by the patient. **With new findings being published on a regular basis**, the problem becomes even more challenging. **A document stating the newest findings might be proved wrong or irrelevant very soon**. LLMs are trained on a large amount of documents to generate the most likely (or probable) response. However, the most probable response might not be the most accurate. This is especially true in the medical domain, where new knowledge, medicine, and procedures are being discovered on a daily basis. Generating the most probable  answers to medical questions might be just wrong and lead to fatal consequences.
	The aim of this project is to explore the possibility of leveraging RAG to increase the performance of medical question answering. Specifically, different inputs (question only, question concatenated documents associated with the question) for answer generation will be tested. The associated performances on medical question answering will be compared. The result of this project can be generalized to any other domain that requires up-to-date information for accurate question answering.

 ## Dataset
